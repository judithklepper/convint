{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8GTvgr7pArM"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import math\n",
    "import psutil\n",
    "import time\n",
    "import datetime\n",
    "from io import open\n",
    "import random\n",
    "from random import shuffle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.cuda\n",
    "\n",
    "import sys; sys.argv=['']; del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xdBOnTnVrecq",
    "outputId": "2764e849-cdb1-4439-f18d-bb6027477965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ACDMcwbu4ZcQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def uniToAscii(sentence):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', sentence)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"Lowercase etc, kan worden uitgebreid naar dataset requirements\"\"\" \n",
    "def normalizeString(s):\n",
    "\n",
    "    s = uniToAscii(s.lower().strip())\n",
    "\n",
    "    return s   \n",
    "\n",
    "'''eventueel te gebruiken als je bepaalde starttoken zinnen te verwijderen'''\n",
    "prefix_filtered = (\n",
    ")\n",
    "\n",
    "\"\"\"Filters pair over max length en eventueel gespecificeerde start characters prefix_filtered\"\"\"\n",
    "def filterPair(p, max_length, start_filter):\n",
    "    filtered = len(p[0].split(' ')) < max_length and \\\n",
    "        len(p[1].split(' ')) < max_length \n",
    "    if start_filter:\n",
    "        return filtered and p[1].startswith(prefixes_filter)\n",
    "    else:\n",
    "        return filtered\n",
    "\n",
    "\"\"\"filter pairs (pytorch tutorial)\"\"\"\n",
    "def filterPairs(pairs, max_length, start_filter):\n",
    "    return [pair for pair in pairs if filterPair(pair, max_length, start_filter)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SeMWu8LUtE5s"
   },
   "outputs": [],
   "source": [
    "\"\"\"start of sentence \"\"\"\n",
    "SOS_token = 0\n",
    "\n",
    "\"\"\"end of sentence\"\"\"\n",
    "EOS_token = 1\n",
    "\n",
    "\"\"\"unknown word\"\"\"\n",
    "UNK_token = 2\n",
    "\n",
    "\n",
    "\"\"\"Lang class, storing in pytorch\"\"\"\n",
    "class Lang:\n",
    "    def __init__(self, language):\n",
    "        self.language_name = language\n",
    "        self.word_to_index = {\"SOS\":SOS_token, \"EOS\":EOS_token, \"<UNK>\":UNK_token}\n",
    "        self.word_to_count = {}\n",
    "        self.index_to_word = {SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"<UNK>\"}\n",
    "        self.vocab_size = 3\n",
    "        self.cutoff_point = -1\n",
    "\n",
    "\n",
    "    def countSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.countWords(word)\n",
    "\n",
    "    \"\"\"aantal woorden in dataset\"\"\"\n",
    "    def countWords(self, word):\n",
    "        if word not in self.word_to_count:\n",
    "            self.word_to_count[word] = 1\n",
    "        else:\n",
    "            self.word_to_count[word] += 1\n",
    "\n",
    "    \"\"\"als max aantal woorden > aantal woorden worden infrequente woorden verwijderd uit vocab\"\"\"\n",
    "    def createCutoff(self, max_vocab_size):\n",
    "        word_freqs = list(self.word_to_count.values())\n",
    "        word_freqs.sort(reverse=True)\n",
    "        if len(word_freqs) > max_vocab_size:\n",
    "            self.cutoff_point = word_freqs[max_vocab_size]\n",
    "\n",
    "    \"\"\"woord -> index\"\"\"\n",
    "    def addSentence(self, sentence):\n",
    "        new_sentence = ''\n",
    "        for word in sentence.split(' '):\n",
    "            unk_word = self.addWord(word)\n",
    "            if not new_sentence:\n",
    "                new_sentence =unk_word\n",
    "            else:\n",
    "                new_sentence = new_sentence + ' ' + unk_word\n",
    "        return new_sentence\n",
    "\n",
    "    \"\"\"woord--> vocab\"\"\" \n",
    "    def addWord(self, word):\n",
    "        if self.word_to_count[word] > self.cutoff_point:\n",
    "            if word not in self.word_to_index:\n",
    "                self.word_to_index[word] = self.vocab_size\n",
    "                self.index_to_word[self.vocab_size] = word\n",
    "                self.vocab_size += 1\n",
    "            return word\n",
    "        else:\n",
    "            return self.index_to_word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fifvzYxufA72"
   },
   "outputs": [],
   "source": [
    "'''prepares input & output naar Lang van .txt dataset'''\n",
    "\n",
    "def prepareLangs(lang1, lang2, file_path, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    lines = open(file_path, encoding='utf-8').\\\n",
    "    read().strip().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08g-wgM6s8CU"
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepareData(lang1, lang2, file_path, max_vocab_size=50000, \n",
    "                reverse=False, trim=0, start_filter=False, perc_train_set=0.9, \n",
    "                print_to=None):\n",
    "    \n",
    "    input_lang, output_lang, pairs = prepareLangs(lang1, lang2, \n",
    "                                                  file_path, reverse)\n",
    "    \n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    if print_to:\n",
    "        with open(print_to,'a') as f:\n",
    "            f.write(\"Read %s sentence pairs \\n\" % len(pairs))\n",
    "    \n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.countSentence(pair[0])\n",
    "        output_lang.countSentence(pair[1])\n",
    "\n",
    "\n",
    "    input_lang.createCutoff(max_vocab_size)\n",
    "    output_lang.createCutoff(max_vocab_size)\n",
    "\n",
    "    pairs = [(input_lang.addSentence(pair[0]),output_lang.addSentence(pair[1])) \n",
    "             for pair in pairs]\n",
    "\n",
    "    shuffle(pairs)\n",
    "    \n",
    "    train_pairs = pairs[:math.ceil(perc_train_set*len(pairs))]\n",
    "    test_pairs = pairs[math.ceil(perc_train_set*len(pairs)):]\n",
    "\n",
    "    print(\"Train pairs: %s\" % (len(train_pairs)))\n",
    "    print(\"Test pairs: %s\" % (len(test_pairs)))\n",
    "    print(\"Counted Words -> Trimmed Vocabulary Sizes (w/ EOS and SOS tags):\")\n",
    "    print(\"%s, %s -> %s\" % (input_lang.language_name, len(input_lang.word_to_count),\n",
    "                            input_lang.vocab_size,))\n",
    "    print(\"%s, %s -> %s\" % (output_lang.language_name, len(output_lang.word_to_count), \n",
    "                            output_lang.vocab_size))\n",
    "    print(random.choice(pairs))\n",
    "\n",
    "    if print_to:\n",
    "        with open(print_to,'a') as f:\n",
    "            f.write(\"Train pairs: %s\" % (len(train_pairs)))\n",
    "            f.write(\"Test pairs: %s\" % (len(test_pairs)))\n",
    "            f.write(\"Counted Words -> Trimmed Vocabulary Sizes (w/ EOS and SOS tags):\")\n",
    "            f.write(\"%s, %s -> %s\" % (input_lang.language_name, \n",
    "                                      len(input_lang.word_to_count),\n",
    "                                      input_lang.vocab_size,))\n",
    "            f.write(\"%s, %s -> %s \\n\" % (output_lang.language_name, len(output_lang.word_to_count), \n",
    "                            output_lang.vocab_size))\n",
    "        \n",
    "    return input_lang, output_lang, train_pairs, test_pairs\n",
    "    return input_lang, output_lang, train_pairs, test_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4Hrm_ortNM2"
   },
   "outputs": [],
   "source": [
    "\"\"\"converts a sentence to one hot encoding vectors\"\"\"\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    indexes = []\n",
    "    for word in sentence.split(' '):\n",
    "        try:\n",
    "            indexes.append(lang.word_to_index[word])\n",
    "        except:\n",
    "            indexes.append(lang.word_to_index[\"<UNK>\"])\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = torch.LongTensor(indexes).view(-1)\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "      \n",
    "\"\"\"converts a pair of sentence (input and target) to a pair of tensors\"\"\"\n",
    "def tensorsFromPair(input_lang, output_lang, pair):\n",
    "    input_variable = tensorFromSentence(input_lang, pair[0])\n",
    "    target_variable = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)\n",
    "  \n",
    "\n",
    "\"\"\"converts from tensor of one hot encoding vector indices to sentence\"\"\"\n",
    "def sentenceFromTensor(lang, tensor):\n",
    "    raw = tensor.data\n",
    "    words = []\n",
    "    for num in raw:\n",
    "        words.append(lang.index_to_word[num.item()])\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nONw2n6X2jHM"
   },
   "outputs": [],
   "source": [
    "\"\"\"verdeling in batches\"\"\"\n",
    "def batchify(data, input_lang, output_lang, batch_size, shuffle_data=True):\n",
    "    if shuffle_data == True:\n",
    "        shuffle(data)\n",
    "    number_of_batches = len(data) // batch_size\n",
    "    batches = list(range(number_of_batches))\n",
    "    longest_elements = list(range(number_of_batches))\n",
    "    \n",
    "    for batch_number in range(number_of_batches):\n",
    "        longest_input = 0\n",
    "        longest_target = 0\n",
    "        input_variables = list(range(batch_size))\n",
    "        target_variables = list(range(batch_size))\n",
    "        index = 0      \n",
    "        for pair in range((batch_number*batch_size),((batch_number+1)*batch_size)):\n",
    "            input_variables[index], target_variables[index] = tensorsFromPair(input_lang, output_lang, data[pair])\n",
    "            if len(input_variables[index]) >= longest_input:\n",
    "                longest_input = len(input_variables[index])\n",
    "            if len(target_variables[index]) >= longest_target:\n",
    "                longest_target = len(target_variables[index])\n",
    "            index += 1\n",
    "        batches[batch_number] = (input_variables, target_variables)\n",
    "        longest_elements[batch_number] = (longest_input, longest_target)\n",
    "    return batches , longest_elements, number_of_batches\n",
    "\n",
    "\n",
    "\"\"\"pads batches to allow for sentences of variable lengths to be computed in parallel\"\"\"\n",
    "def pad_batch(batch):\n",
    "    padded_inputs = torch.nn.utils.rnn.pad_sequence(batch[0],padding_value=EOS_token)\n",
    "    padded_targets = torch.nn.utils.rnn.pad_sequence(batch[1],padding_value=EOS_token)\n",
    "    return (padded_inputs, padded_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6a1ErP7tGTN"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "\tdef __init__(self,input_size,hidden_size,layers=1,dropout=0.1,\n",
    "               bidirectional=True):\n",
    "\t\tsuper(EncoderRNN, self).__init__()\n",
    "\n",
    "\t\tif bidirectional:\n",
    "\t\t\tself.directions = 2\n",
    "\t\telse:\n",
    "\t\t\tself.directions = 1\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.num_layers = layers\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.embedder = nn.Embedding(input_size,hidden_size)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\tself.lstm = nn.LSTM(input_size=hidden_size,hidden_size=hidden_size,\n",
    "                        num_layers=layers,dropout=dropout,\n",
    "                        bidirectional=bidirectional,batch_first=False)\n",
    "\t\tself.fc = nn.Linear(hidden_size*self.directions, hidden_size)\n",
    "\n",
    "\tdef forward(self, input_data, h_hidden, c_hidden):\n",
    "\t\tembedded_data = self.embedder(input_data)\n",
    "\t\tembedded_data = self.dropout(embedded_data)\n",
    "\t\thiddens, outputs = self.lstm(embedded_data, (h_hidden, c_hidden))\n",
    "\n",
    "\t\treturn hiddens, outputs\n",
    "\n",
    "\t\"\"\"creates initial hidden states for encoder corresponding to batch size\"\"\"\n",
    "\tdef create_init_hiddens(self, batch_size):\n",
    "\t\th_hidden = Variable(torch.zeros(self.num_layers*self.directions, \n",
    "                                    batch_size, self.hidden_size))\n",
    "\t\tc_hidden = Variable(torch.zeros(self.num_layers*self.directions, \n",
    "                                    batch_size, self.hidden_size))\n",
    "\t\tif torch.cuda.is_available():\n",
    "\t\t\treturn h_hidden.cuda(), c_hidden.cuda()\n",
    "\t\telse:\n",
    "\t\t\treturn h_hidden, c_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCgWDqL8txLp"
   },
   "outputs": [],
   "source": [
    "class DecoderAttn(nn.Module):\n",
    "\tdef __init__(self, hidden_size, output_size, layers=1, dropout=0.1, bidirectional=True):\n",
    "\t\tsuper(DecoderAttn, self).__init__()\n",
    "\n",
    "\t\tif bidirectional:\n",
    "\t\t\tself.directions = 2\n",
    "\t\telse:\n",
    "\t\t\tself.directions = 1\n",
    "\t\tself.output_size = output_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.num_layers = layers\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.embedder = nn.Embedding(output_size,hidden_size)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\tself.score_learner = nn.Linear(hidden_size*self.directions, \n",
    "                                   hidden_size*self.directions)\n",
    "\t\tself.lstm = nn.LSTM(input_size=hidden_size,hidden_size=hidden_size,\n",
    "                        num_layers=layers,dropout=dropout,\n",
    "                        bidirectional=bidirectional,batch_first=False)\n",
    "\t\tself.context_combiner = nn.Linear((hidden_size*self.directions)\n",
    "                                      +(hidden_size*self.directions), hidden_size)\n",
    "\t\tself.tanh = nn.Tanh()\n",
    "\t\tself.output = nn.Linear(hidden_size, output_size)\n",
    "\t\tself.soft = nn.Softmax(dim=1)\n",
    "\t\tself.log_soft = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "\tdef forward(self, input_data, h_hidden, c_hidden, encoder_hiddens):\n",
    "\n",
    "\t\tembedded_data = self.embedder(input_data)\n",
    "\t\tembedded_data = self.dropout(embedded_data)\t\n",
    "\t\tbatch_size = embedded_data.shape[1]\n",
    "\t\thiddens, outputs = self.lstm(embedded_data, (h_hidden, c_hidden))\t\n",
    "\t\ttop_hidden = outputs[0].view(self.num_layers,self.directions,\n",
    "                                 hiddens.shape[1],\n",
    "                                 self.hidden_size)[self.num_layers-1]\n",
    "\t\ttop_hidden = top_hidden.permute(1,2,0).contiguous().view(batch_size,-1, 1)\n",
    "\n",
    "\t\tprep_scores = self.score_learner(encoder_hiddens.permute(1,0,2))\n",
    "\t\tscores = torch.bmm(prep_scores, top_hidden)\n",
    "\t\tattn_scores = self.soft(scores)\n",
    "\t\tcon_mat = torch.bmm(encoder_hiddens.permute(1,2,0),attn_scores)\n",
    "\t\th_tilde = self.tanh(self.context_combiner(torch.cat((con_mat,\n",
    "                                                         top_hidden),dim=1)\n",
    "                                              .view(batch_size,-1)))\n",
    "\t\tpred = self.output(h_tilde)\n",
    "\t\tpred = self.log_soft(pred)\n",
    "\n",
    "\t\t\n",
    "\t\treturn pred, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "344LVWD1VXFF"
   },
   "outputs": [],
   "source": [
    "'''training per batch'''\n",
    "\n",
    "def train_batch(input_batch, target_batch, encoder, decoder, \n",
    "                encoder_optimizer, decoder_optimizer, loss_criterion):\n",
    "\tencoder_optimizer.zero_grad()\n",
    "\tdecoder_optimizer.zero_grad()\n",
    "\tloss = 0\n",
    "\tenc_h_hidden, enc_c_hidden = encoder.create_init_hiddens(input_batch.shape[1])\n",
    "\n",
    "\tenc_hiddens, enc_outputs = encoder(input_batch, enc_h_hidden, enc_c_hidden)\n",
    "\n",
    "\tdecoder_input = Variable(torch.LongTensor(1,input_batch.shape[1]).\n",
    "                           fill_(output_lang.word_to_index.get(\"SOS\")).cuda()) if use_cuda \\\n",
    "\t\t\t\t\telse Variable(torch.LongTensor(1,input_batch.shape[1]).\n",
    "                        fill_(output_lang.word_to_index.get(\"SOS\")))\n",
    "\n",
    "\tdec_h_hidden = enc_outputs[0]\n",
    "\tdec_c_hidden = enc_outputs[1]\n",
    "\t\n",
    "\tfor i in range(target_batch.shape[0]):\n",
    "\t\tpred, dec_outputs = decoder(decoder_input, dec_h_hidden, \n",
    "                                dec_c_hidden, enc_hiddens)\n",
    "\n",
    "\t\tdecoder_input = target_batch[i].view(1,-1)\n",
    "\t\tdec_h_hidden = dec_outputs[0]\n",
    "\t\tdec_c_hidden = dec_outputs[1]\n",
    "\t\t\n",
    "\t\tloss += loss_criterion(pred,target_batch[i])\n",
    "\n",
    "\n",
    "\tloss.backward()\n",
    "\n",
    "\ttorch.nn.utils.clip_grad_norm_(encoder.parameters(),args.clip)\n",
    "\ttorch.nn.utils.clip_grad_norm_(decoder.parameters(),args.clip)\n",
    "\n",
    "\tencoder_optimizer.step()\n",
    "\tdecoder_optimizer.step()\n",
    "\n",
    "\treturn loss.item() / target_batch.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_AkStAnhVYs-"
   },
   "outputs": [],
   "source": [
    "''' training epochs'''\n",
    "def train(train_batches, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_criterion):\n",
    "\n",
    "\tround_loss = 0\n",
    "\ti = 1\n",
    "\tfor batch in train_batches:\n",
    "\t\ti += 1\n",
    "\t\t(input_batch, target_batch) = pad_batch(batch)\n",
    "\t\tbatch_loss = train_batch(input_batch, target_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_criterion)\n",
    "\t\tround_loss += batch_loss\n",
    "\n",
    "\treturn round_loss / len(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xdu2FER-Vcnk"
   },
   "outputs": [],
   "source": [
    "'''Evaluate'''\n",
    "\n",
    "def test_batch(input_batch, target_batch, encoder, decoder, loss_criterion):\n",
    "\t\n",
    "\tloss = 0\n",
    "\n",
    "\t#create initial hidde state for encoder\n",
    "\tenc_h_hidden, enc_c_hidden = encoder.create_init_hiddens(input_batch.shape[1])\n",
    "\n",
    "\tenc_hiddens, enc_outputs = encoder(input_batch, enc_h_hidden, enc_c_hidden)\n",
    "\n",
    "\tdecoder_input = Variable(torch.LongTensor(1,input_batch.shape[1]).\n",
    "                           fill_(output_lang.word_to_index.get(\"SOS\")).cuda()) if use_cuda \\\n",
    "\t\t\t\t\telse Variable(torch.LongTensor(1,input_batch.shape[1]).\n",
    "                        fill_(output_lang.word_to_index.get(\"SOS\")))\n",
    "\tdec_h_hidden = enc_outputs[0]\n",
    "\tdec_c_hidden = enc_outputs[1]\n",
    "\t\n",
    "\tfor i in range(target_batch.shape[0]):\n",
    "\t\tpred, dec_outputs = decoder(decoder_input, dec_h_hidden, dec_c_hidden, enc_hiddens)\n",
    "\n",
    "\t\ttopv, topi = pred.topk(1,dim=1)\n",
    "\t\tni = topi.view(1,-1)\n",
    "\t\t\n",
    "\t\tdecoder_input = ni\n",
    "\t\tdec_h_hidden = dec_outputs[0]\n",
    "\t\tdec_c_hidden = dec_outputs[1]\n",
    "\n",
    "\t\tloss += loss_criterion(pred,target_batch[i])\n",
    "\t\t\n",
    "\treturn loss.item() / target_batch.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9bKcK3hVd-F"
   },
   "outputs": [],
   "source": [
    "'''Loss overtest_batches'''\n",
    "\n",
    "def test(test_batches, encoder, decoder, loss_criterion):\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\ttest_loss = 0\n",
    "\n",
    "\t\tfor batch in test_batches:\n",
    "\t\t\t(input_batch, target_batch) = pad_batch(batch)\n",
    "\t\t\tbatch_loss = test_batch(input_batch, target_batch, encoder, decoder, loss_criterion)\n",
    "\t\t\ttest_loss += batch_loss\n",
    "\n",
    "\treturn test_loss / len(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tBxl0KlVjnE"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(encoder, decoder, sentence, cutoff_length):\n",
    "\twith torch.no_grad():\n",
    "\t\tinput_variable = tensorFromSentence(input_lang, sentence)\n",
    "\t\tinput_variable = input_variable.view(-1,1)\n",
    "\t\tenc_h_hidden, enc_c_hidden = encoder.create_init_hiddens(1)\n",
    "\n",
    "\t\tenc_hiddens, enc_outputs = encoder(input_variable, enc_h_hidden, enc_c_hidden)\n",
    "\n",
    "\t\tdecoder_input = Variable(torch.LongTensor(1,1).fill_(output_lang.word_to_index.get(\"SOS\")).cuda()) if use_cuda \\\n",
    "\t\t\t\t\t\telse Variable(torch.LongTensor(1,1).fill_(output_lang.word_to_index.get(\"SOS\")))\n",
    "\t\tdec_h_hidden = enc_outputs[0]\n",
    "\t\tdec_c_hidden = enc_outputs[1]\n",
    "\n",
    "\t\tdecoded_words = []\n",
    "\n",
    "\t\tfor di in range(cutoff_length):\n",
    "\t\t\tpred, dec_outputs = decoder(decoder_input, dec_h_hidden, dec_c_hidden, enc_hiddens)\n",
    "\n",
    "\t\t\ttopv, topi = pred.topk(1,dim=1)\n",
    "\t\t\tni = topi.item()\n",
    "\t\t\tif ni == output_lang.word_to_index.get(\"EOS\"):\n",
    "\t\t\t\tdecoded_words.append('<EOS>')\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tdecoded_words.append(output_lang.index_to_word[ni])\n",
    "\n",
    "\t\t\tdecoder_input = Variable(torch.LongTensor(1,1).fill_(ni).cuda()) if use_cuda \\\n",
    "\t\t\t\t\t\t\telse Variable(torch.LongTensor(1,1).fill_(ni))\n",
    "\t\t\tdec_h_hidden = dec_outputs[0]\n",
    "\t\t\tdec_c_hidden = dec_outputs[1]\n",
    "\n",
    "\t\toutput_sentence = ' '.join(decoded_words)\n",
    "    \n",
    "\t\treturn output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_BVKYuEVk-S"
   },
   "outputs": [],
   "source": [
    "'''Evaluate: format\n",
    "                  > input sentence\n",
    "                  = correct translation\n",
    "                  < predicted translation'''\n",
    "\n",
    "def evaluate_randomly(encoder, decoder, pairs, n=2, trim=100):\n",
    "\tfor i in range(n):\n",
    "\t\tpair = random.choice(pairs)\n",
    "\t\tprint('>', pair[0])\n",
    "\t\tprint('=', pair[1])\n",
    "\t\toutput_sentence = evaluate(encoder, decoder, pair[0], cutoff_length=trim)\n",
    "\t\tprint('<', output_sentence)\n",
    "\t\tprint('')    \n",
    "\t\tif create_txt:\n",
    "\t\t\tf = open(print_to, 'a')\n",
    "\t\t\tf.write(\"\\n \\\n",
    "\t\t\t\t> %s \\n \\\n",
    "\t\t\t\t= %s \\n \\\n",
    "\t\t\t\t< %s \\n\" % (pair[0], pair[1], output_sentence))\n",
    "\t\t\tf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_N2hD47WTf_"
   },
   "outputs": [],
   "source": [
    "'''Used to plot the progress of training. Plots the loss value vs. time'''\n",
    "def showPlot(times, losses, fig_name):\n",
    "    x_axis_label = 'Minutes'\n",
    "    colors = ('red','blue')\n",
    "    if max(times) >= 120:\n",
    "    \ttimes = [mins/60 for mins in times]\n",
    "    \tx_axis_label = 'Hours'\n",
    "    i = 0\n",
    "    for key, losses in losses.items():\n",
    "    \tif len(losses) > 0:\n",
    "    \t\tplt.plot(times, losses, label=key, color=colors[i])\n",
    "    \t\ti += 1\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel(x_axis_label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Results')\n",
    "    plt.savefig(fig_name+'.png')\n",
    "    plt.close('all')\n",
    "    \n",
    "'''prints the current memory consumption'''\n",
    "def mem():\n",
    "\tif use_cuda:\n",
    "\t\tmem = torch.cuda.memory_allocated()/1e7\n",
    "\telse:\n",
    "\t\tmem = psutil.cpu_percent()\n",
    "\tprint('Current mem usage:')\n",
    "\tprint(mem)\n",
    "\treturn \"Current mem usage: %s \\n\" % (mem)\n",
    "\n",
    "'''converts a time measurement in seconds to hours'''\n",
    "def asHours(s):\n",
    "\tm = math.floor(s / 60)\n",
    "\th = math.floor(m / 60)\n",
    "\ts -= m * 60\n",
    "\tm -= h * 60\n",
    "\treturn '%dh %dm %ds' % (h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "701CcLqgWYrE"
   },
   "outputs": [],
   "source": [
    "def train_and_test(epochs, test_eval_every, plot_every, learning_rate, \n",
    "                   lr_schedule, train_pairs, test_pairs, input_lang, \n",
    "                   output_lang, batch_size, test_batch_size, encoder, decoder, \n",
    "                   loss_criterion, trim, save_weights):\n",
    "\t\n",
    "\ttimes = []\n",
    "\tlosses = {'train set':[], 'test set': []}\n",
    "\n",
    "\ttest_batches, longest_seq, n_o_b = batchify(test_pairs, input_lang, \n",
    "                                              output_lang, test_batch_size, \n",
    "                                              shuffle_data=False)\n",
    "\n",
    "\tstart = time.time()\n",
    "\tfor i in range(1,epochs+1):\n",
    "    \n",
    "\t\t'''adjust the learning rate according to the learning rate schedule\n",
    "\t\tspecified in lr_schedule'''\n",
    "\t\tif i in lr_schedule.keys():\n",
    "\t\t\tlearning_rate /= lr_schedule.get(i)\n",
    "\n",
    "\n",
    "\t\tencoder.train()\n",
    "\t\tdecoder.train()\n",
    "\n",
    "\t\tencoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "\t\tdecoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "\t\tbatches, longest_seq, n_o_b = batchify(train_pairs, input_lang, \n",
    "                                           output_lang, batch_size, \n",
    "                                           shuffle_data=True)\n",
    "\t\ttrain_loss = train(batches, encoder, decoder, encoder_optimizer, \n",
    "                       decoder_optimizer, loss_criterion)\n",
    "\t\t\n",
    "\t\tnow = time.time()\n",
    "\t\tprint(\"Iter: %s \\nLearning Rate: %s \\nTime: %s \\nTrain Loss: %s \\n\" \n",
    "          % (i, learning_rate, asHours(now-start), train_loss))\n",
    "\n",
    "\t\tif create_txt:\n",
    "\t\t\twith open(print_to, 'a') as f:\n",
    "\t\t\t\tf.write(\"Iter: %s \\nLeaning Rate: %s \\nTime: %s \\nTrain Loss: %s \\n\" \\\n",
    "\t\t\t\t\t% (i, learning_rate, asHours(now-start), train_loss))\n",
    "\n",
    "\t\tif i % test_eval_every == 0:\n",
    "\t\t\tif test_pairs:\n",
    "\t\t\t\ttest_loss = test(test_batches, encoder, decoder, criterion)\n",
    "\t\t\t\tprint(\"Test set loss: %s\" % (test_loss))\n",
    "\t\t\t\tif create_txt:\n",
    "\t\t\t\t\twith open(print_to, 'a') as f:\n",
    "\t\t\t\t\t\tf.write(\"Test Loss: %s \\n\" % (test_loss))\n",
    "\t\t\t\tevaluate_randomly(encoder, decoder, test_pairs)\n",
    "\t\t\telse:\n",
    "\t\t\t\tevaluate_randomly(encoder, decoder, train_pairs)\n",
    "\n",
    "\t\tif i % plot_every == 0:\n",
    "\t\t\ttimes.append((time.time()-start)/60)\n",
    "\t\t\tlosses['train set'].append(train_loss)\n",
    "\t\t\tif test_pairs:\n",
    "\t\t\t\tlosses['test set'].append(test_loss)\n",
    "\t\t\tshowPlot(times, losses, output_file_name)\n",
    "\t\t\tif save_weights:\n",
    "\t\t\t\ttorch.save(encoder.state_dict(), output_file_name+'_enc_weights.pt')\n",
    "\t\t\t\ttorch.save(decoder.state_dict(), output_file_name+'_dec_weights.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yqYvI7Xt-Ot"
   },
   "outputs": [],
   "source": [
    "'''Dataset en parametrs inladen'''\n",
    "file_path = 'data/prefix-equation611.txt'\n",
    "input_lang_name = 'equation'\n",
    "output_lang_name = 'prefix'\n",
    "\n",
    "\"\"\"name of your dataset\"\"\"\n",
    "dataset = 'orig'\n",
    "\n",
    "'''kan hier ook twee files inladen als je talen gescheiden zijn'''\n",
    "raw_data_file_path = (file_path)\n",
    "\n",
    "\"\"\"True als je richting van vertaling wilt omdraaien bijv eng-fra naar fra-eng\"\"\"\n",
    "reverse=True\n",
    "\n",
    "'''verwijder zinnen met hoeveel woorden? Hier niet echt van toepassing, maar voor Math word problem'''\n",
    "trim = 100\n",
    "\n",
    "max_vocab_size= 1000\n",
    "\n",
    "\"\"\"start filter kan gebruikt worden om bijvoorbeeld alle paren te verwijderen met een bepaalde prefix,\n",
    "   hier niet gespecificeerd (van pytorch tutorial)\"\"\"\n",
    "start_filter = False\n",
    "\n",
    "\"\"\"train/test split (procent training data)\"\"\"\n",
    "perc_train_set = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7aTcns-8HNT-"
   },
   "outputs": [],
   "source": [
    "\"\"\"OUTPUT OPTIONS\"\"\"\n",
    "\n",
    "\"\"\"om hoeveeel epochs losse berekenen\"\"\"\n",
    "test_eval_every = 1\n",
    "\n",
    "\"\"\"hoe vaak plotten\"\"\"\n",
    "plot_every = 1\n",
    "\n",
    "\"\"\"txt file met output\"\"\"\n",
    "create_txt = True\n",
    "\n",
    "\"\"\"if true saves the encoder and decoder weights to seperate .pt files e\"\"\"\n",
    "save_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MC4-4tIkWuqp"
   },
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "\n",
    "\"\"\"bidirectional LSTM of één richting, in geval van vertaling bidirectional\"\"\"\n",
    "bidirectional = True\n",
    "if bidirectional:\n",
    "\tdirections = 2\n",
    "else:\n",
    "\tdirections = 1\n",
    "\n",
    "\n",
    "layers = 2\n",
    "hidden_size = 100\n",
    "dropout = 0.2\n",
    "\n",
    "batch_size = 32\n",
    "test_batch_size = 32\n",
    "\n",
    "epochs = 5\n",
    "learning_rate= 0.1\n",
    "\n",
    "\"\"\"Learning rate schedule. Met dit schema na 5 epochs learning rate gedeeld door 10\"\"\"\n",
    "lr_schedule = {5:10}\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KSB24rtUW1I9",
    "outputId": "5079cb90-b9c6-41f7-ac25-09275356b517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 578 sentence pairs\n",
      "Counting words...\n",
      "Train pairs: 463\n",
      "Test pairs: 115\n",
      "Counted Words -> Trimmed Vocabulary Sizes (w/ EOS and SOS tags):\n",
      "prefix, 65 -> 68\n",
      "equation, 29 -> 32\n",
      "('how many is 9 * 6 times 1?', '* (* 9 6) 1')\n",
      "Train Pairs #\n",
      "463\n",
      "Current mem usage:\n",
      "44.0\n",
      "Current mem usage:\n",
      "0.0\n",
      "Encoder and Decoder Created\n",
      "Current mem usage:\n",
      "100.0\n",
      "Number of epochs: 5\n",
      "Current mem usage:\n",
      "0.0\n",
      "Iter: 1 \n",
      "Learning Rate: 0.1 \n",
      "Time: 0h 0m 3s \n",
      "Train Loss: 3.4224116915748235 \n",
      "\n",
      "Test set loss: 3.3805842929416237\n",
      "> i want to calculate nine / 6 divided by ten?\n",
      "= / (/ 9 6) 10\n",
      "< <EOS>\n",
      "\n",
      "> how many is five divided by seven multiplied by 1?\n",
      "= * (/ 5 7) 1\n",
      "< <EOS>\n",
      "\n",
      "Iter: 2 \n",
      "Learning Rate: 0.1 \n",
      "Time: 0h 0m 7s \n",
      "Train Loss: 3.3483253660656156 \n",
      "\n",
      "Test set loss: 3.3007528516981335\n",
      "> what is nine times 10 - seven?\n",
      "= - (* 9 10) 7\n",
      "< <EOS>\n",
      "\n",
      "> i would like to know what is two plus seven + ten?\n",
      "= + (+ 2 7) 10\n",
      "< <EOS>\n",
      "\n",
      "Iter: 3 \n",
      "Learning Rate: 0.1 \n",
      "Time: 0h 0m 13s \n",
      "Train Loss: 3.2744802066258023 \n",
      "\n",
      "Test set loss: 3.2282914055718313\n",
      "> i would like to know what is 2 times one extracted by two?\n",
      "= - (* 2 1) 2\n",
      "< <EOS>\n",
      "\n",
      "> what is nine / 9 added with 1?\n",
      "= + (/ 9 9) 1\n",
      "< <EOS>\n",
      "\n",
      "Iter: 4 \n",
      "Learning Rate: 0.1 \n",
      "Time: 0h 0m 17s \n",
      "Train Loss: 3.2160519191196983 \n",
      "\n",
      "Test set loss: 3.179356575012207\n",
      "> i would like to know what is six * 9 extracted by two?\n",
      "= - (* 6 9) 2\n",
      "< <EOS>\n",
      "\n",
      "> how many is 8 extracted by eight plus 10?\n",
      "= + (- 8 8) 10\n",
      "< <EOS>\n",
      "\n",
      "Iter: 5 \n",
      "Learning Rate: 0.01 \n",
      "Time: 0h 0m 22s \n",
      "Train Loss: 3.186219873882475 \n",
      "\n",
      "Test set loss: 3.17462518480089\n",
      "> what is five - 4 extracted by 6?\n",
      "= - (- 5 4) 6\n",
      "< <EOS>\n",
      "\n",
      "> how many is nine / 6 times 8?\n",
      "= * (/ 9 6) 8\n",
      "< <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "\"\"\"for plotting of the loss\"\"\"\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "output_file_name = \"testdata.%s_trim.%s_vocab.%s_directions.%s_layers.%s_hidden.%s_dropout.%s_learningrate.%s_batch.%s_epochs.%s\" % (dataset,trim,max_vocab_size,directions,layers,hidden_size,dropout,learning_rate,batch_size,epochs)\n",
    "\n",
    "if create_txt:\n",
    "\tprint_to = output_file_name+'.txt'\n",
    "\twith open(print_to, 'w+') as f:\n",
    "\t\tf.write(\"Starting Training \\n\")\n",
    "else:\n",
    "\tprint_to = None\n",
    "\n",
    "input_lang, output_lang, train_pairs, test_pairs = prepareData(\n",
    "    input_lang_name, output_lang_name, raw_data_file_path, \n",
    "    max_vocab_size=max_vocab_size, reverse=reverse, trim=trim, \n",
    "    start_filter=start_filter, perc_train_set=perc_train_set, print_to=print_to)\n",
    "print('Train Pairs #')\n",
    "print(len(train_pairs))\n",
    "\n",
    "\n",
    "\"\"\"for gradient clipping from \n",
    "https://github.com/pytorch/examples/blob/master/word_language_model/main.py\"\"\"\n",
    "parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM Language Model')\n",
    "parser.add_argument('--clip', type=float, default=0.25,\n",
    "                    help='gradient clipping')\n",
    "args = parser.parse_args()\n",
    "\n",
    "mem()\n",
    "\n",
    "if create_txt:\n",
    "\twith open(print_to, 'a') as f:\n",
    "\t\tf.write(\"\\nRandom Train Pair: %s \\n\\nRandom Test Pair: %s \\n\\n\" \n",
    "            % (random.choice(train_pairs),random.choice(test_pairs) \n",
    "               if test_pairs else \"None\"))\n",
    "\t\tf.write(mem())\n",
    "\n",
    "\n",
    "\"\"\"create the Encoder\"\"\"\n",
    "encoder = EncoderRNN(input_lang.vocab_size, hidden_size, layers=layers, \n",
    "                     dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "\"\"\"create the Decoder\"\"\"\n",
    "decoder = DecoderAttn(hidden_size, output_lang.vocab_size, layers=layers, \n",
    "                      dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "print('Encoder and Decoder Created')\n",
    "mem()\n",
    "\n",
    "if use_cuda:\n",
    "\tprint('Cuda being used')\n",
    "\tencoder = encoder.cuda()\n",
    "\tdecoder = decoder.cuda()\n",
    "\n",
    "print('Number of epochs: '+str(epochs))\n",
    "\n",
    "if create_txt:\n",
    "\twith open(print_to, 'a') as f:\n",
    "\t\tf.write('Encoder and Decoder Created\\n')\n",
    "\t\tf.write(mem())\n",
    "\t\tf.write(\"Number of epochs %s \\n\" % (epochs))\n",
    "\n",
    "train_and_test(epochs, test_eval_every, plot_every, learning_rate, lr_schedule, \n",
    "               train_pairs, test_pairs, input_lang, output_lang, batch_size, \n",
    "               test_batch_size, encoder, decoder, criterion, trim, save_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kv8oh5K6wYWD"
   },
   "outputs": [],
   "source": [
    "outside_sent = \"what is one plus two times three\"\n",
    "outside_sent = normalizeString(outside_sent)\n",
    "evaluate(encoder, decoder, outside_sent, cutoff_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
